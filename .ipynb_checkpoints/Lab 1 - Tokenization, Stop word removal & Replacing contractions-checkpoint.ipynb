{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Tokenizing the text</h1>\n",
    "<p>First step when we have a string is to tokenize is. Both sentence tokenization and word tokenization can be done using in-built functions from nltk. Importing the required functions from nltk for tokenizing.</p>\n",
    "<p>Next we initialise a string which we will tokenize. The string will contain multiple sentences using delimiters.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "text = 'Hello World!!! This is Dr. Smith. How do you do? I am good.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Sentence Tokenization</h2>\n",
    "<p> As seen below the sentences are divided based on delimiters such as . ? !<br />\n",
    "    But it wont consider just 'Dr.' as a seperate string just because of the '.'<br />\n",
    "    Hence this is not blindly splitting on the base of delimiters<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello World!!!', 'This is Dr. Smith.', 'How do you do?', 'I am good.']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Word Tokenization</h3>\n",
    "<p>Here the words are divided on the basis of spaces and each of the delimiters are considered to be seperate words.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World', '!', '!', '!', 'This', 'is', 'Dr.', 'Smith', '.', 'How', 'do', 'you', 'do', '?', 'I', 'am', 'good', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Word Removal\n",
    "* Now we remove stop words from the text given.\n",
    "* NLTK provides us with multiple sets of stop words which we can use so as to filter our text by removing them.\n",
    "* We will first take only the english stop words from NLTK's stop words.\n",
    "* Initialize a string with a sentence.\n",
    "* Apply word tokenization on it to get a list of the words.\n",
    "* Next we filter the words and remove all the stop words from that list using the stop words we got from NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'from', 'some', 'should', 'what', 'them', 'd', 'once', 'do', 'below', 'ma', 'above', 'themselves', 'your', 'further', 'between', 'no', 'doesn', 'his', 'yours', \"haven't\", \"couldn't\", 'very', 'to', 'him', 'if', 'before', 'all', 'shouldn', 'those', 'here', \"shouldn't\", 'doing', 'y', 'himself', 's', 'out', 'of', 'by', 'few', 'which', \"mightn't\", 'am', 'myself', 'only', 'm', 'but', 'didn', 'are', 'have', 'more', 'just', 'through', 'itself', 'off', 'isn', 'both', \"you've\", 'into', 'it', 'who', 'during', \"that'll\", 'having', 'had', 'has', 'as', 'our', 'my', 'again', 'than', 'down', 'such', 'these', 'too', 'against', 'hasn', 'how', 'over', 'other', \"weren't\", 'be', 'nor', 'will', \"needn't\", 'because', \"hadn't\", \"wasn't\", \"you'd\", 'haven', 'they', \"didn't\", 'whom', 'there', 'so', 'a', 'hers', \"should've\", 'you', \"it's\", 've', 't', 'after', 'herself', \"don't\", 'any', 'under', 'when', 'was', 'and', \"doesn't\", 'shan', \"you'll\", 'yourselves', 'mightn', 'ours', 'while', 'does', \"isn't\", 'aren', \"aren't\", 'same', 'mustn', 'she', 'with', 'their', 'then', 'yourself', 'not', 'couldn', 'needn', 'were', 'll', 'at', 'an', 'i', 'about', 'the', 'me', 'wouldn', 'he', 'theirs', 'or', 'been', 'being', 'where', 'for', 'up', 'is', 'can', 'her', 'did', \"mustn't\", 'now', 'ain', 'don', 'that', 'most', 'ourselves', \"you're\", \"wouldn't\", 'on', 'we', 'in', 'each', \"hasn't\", 'o', 'weren', 'hadn', 'wasn', 'its', 'own', 'won', 're', \"won't\", \"shan't\", 'why', 'this', \"she's\", 'until'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "print(stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', 'used', 'to', 'show', 'off', 'the', 'stop', 'words', 'filtering']\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a sample sentence used to show off the stop words filtering\"\n",
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample', 'sentence', 'used', 'show', 'stop', 'words', 'filtering']\n"
     ]
    }
   ],
   "source": [
    "filtered = []\n",
    "for w in words:\n",
    "    if w.lower() not in stops:\n",
    "        filtered.append(w)\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replacing contractions\n",
    "Contractions are shorthands used for contracting two words into one. For example \"didn't\" is a contraction for \"did not\"\n",
    "* To remove contractions we create a dictionary with the keys as the contractions and the values as their expansions\n",
    "* Next we split the text into words and replce the text with their expansions wherever required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', \"don't\", 'agree', 'with', 'this', 'cause', 'I', \"ain't\", 'convinced']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contractions = {\n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\"}\n",
    "text = \"I don't agree with this cause I ain't convinced\"\n",
    "text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'do', 'not', 'agree', 'with', 'this', 'because', 'I', 'am', 'not', 'convinced']\n"
     ]
    }
   ],
   "source": [
    "for w in text.split():\n",
    "    if w.lower() in contractions:\n",
    "        text = text.replace(w, contractions[w.lower()])\n",
    "print(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
